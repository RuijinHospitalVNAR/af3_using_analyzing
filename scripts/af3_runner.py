#!/usr/bin/env python3
"""
AlphaFold3 æ‰¹é‡è¿è¡Œå·¥å…·
=====================
ç»Ÿä¸€å·¥å…·ç”¨äºæ‰¹é‡è¿è¡Œ AlphaFold3 é¢„æµ‹ï¼Œæ”¯æŒï¼š
1. å•æ¬¡è¿è¡Œ
2. å¹¶è¡Œæ‰¹é‡è¿è¡Œ
3. GPU/CPU è‡ªåŠ¨æ£€æµ‹å’Œåˆ†é…
4. è¿›åº¦ç›‘æ§å’Œæ—¥å¿—è®°å½•

ä½¿ç”¨æ–¹æ³•:
    python af3_runner.py --input_dir /path/to/json_files --output_dir /path/to/output

ç¤ºä¾‹:
    python af3_runner.py --input_dir ./inputs --output_dir ./results --max_concurrent 4
"""

import os
import sys
import json
import logging
import argparse
import subprocess
import threading
import time
from pathlib import Path
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed


def setup_logging(verbose: bool = False):
    """é…ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )


def detect_gpu_count() -> int:
    """æ£€æµ‹å¯ç”¨GPUæ•°é‡"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_count = len(result.stdout.strip().split('\n'))
            logging.info(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
            return gpu_count
    except FileNotFoundError:
        logging.info("æœªæ£€æµ‹åˆ°nvidia-smiï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
    return 0


def validate_json_file(json_path: str) -> bool:
    """éªŒè¯JSONæ–‡ä»¶æ ¼å¼"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µï¼ˆAF3 å®˜æ–¹è¾“å…¥ï¼‰
        required_fields = ['sequences', 'dialect', 'version']
        for field in required_fields:
            if field not in data:
                logging.error(f"JSONæ–‡ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        if data.get('dialect') != 'alphafold3':
            logging.error("dialect å¿…é¡»ä¸º 'alphafold3'")
            return False
        
        # æ£€æŸ¥åºåˆ—æ ¼å¼
        sequences = data['sequences']
        if not isinstance(sequences, list) or len(sequences) == 0:
            logging.error("sequenceså¿…é¡»æ˜¯éç©ºåˆ—è¡¨")
            return False
        
        for i, seq in enumerate(sequences):
            if not isinstance(seq, dict):
                logging.error(f"åºåˆ— {i} æ ¼å¼é”™è¯¯")
                return False
            
            # æ£€æŸ¥è›‹ç™½è´¨æˆ–RNAåºåˆ—ï¼ˆæœ€å¸¸è§ä¸º proteinï¼‰
            if 'protein' in seq:
                protein = seq['protein']
                if 'sequence' not in protein:
                    logging.error(f"åºåˆ— {i} ç¼ºå°‘sequenceå­—æ®µ")
                    return False
                # éªŒè¯æ°¨åŸºé…¸åºåˆ—ï¼ˆå…è®¸å¤§å†™ï¼‰
                sequence = str(protein['sequence']).upper()
                valid_aa = set("ACDEFGHIKLMNPQRSTVWY")
                if not all(aa in valid_aa for aa in sequence):
                    logging.error(f"åºåˆ— {i} åŒ…å«æ— æ•ˆæ°¨åŸºé…¸")
                    return False
            elif 'rna' in seq:
                rna = seq['rna']
                if 'sequence' not in rna:
                    logging.error(f"RNA åºåˆ— {i} ç¼ºå°‘sequenceå­—æ®µ")
                    return False
            else:
                logging.error(f"åºåˆ— {i} æ—¢ä¸åŒ…å« protein ä¹Ÿä¸åŒ…å« rna å­—æ®µ")
                return False
        
        logging.debug(f"JSONæ–‡ä»¶éªŒè¯é€šè¿‡: {json_path}")
        return True
        
    except json.JSONDecodeError as e:
        logging.error(f"JSONè§£æé”™è¯¯ {json_path}: {e}")
        return False
    except Exception as e:
        logging.error(f"éªŒè¯æ–‡ä»¶æ—¶å‡ºé”™ {json_path}: {e}")
        return False


def run_single_prediction(
    json_file: str,
    output_dir: str,
    af3_script: str,
    model_dir: str,
    public_db_dir: Optional[str] = None,
    run_data_pipeline: bool = True,
    run_inference: bool = True,
    gpu_id: Optional[int] = None,
    extra_args: Optional[List[str]] = None,
) -> Dict[str, any]:
    """
    è¿è¡Œå•ä¸ªAlphaFold3é¢„æµ‹
    
    Args:
        json_file: JSONè¾“å…¥æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        af3_script: AF3è„šæœ¬è·¯å¾„
        max_template_date: æœ€å¤§æ¨¡æ¿æ—¥æœŸ
        gpu_id: GPU IDï¼ˆNoneè¡¨ç¤ºCPUæ¨¡å¼ï¼‰
        
    Returns:
        è¿è¡Œç»“æœå­—å…¸
    """
    base_name = os.path.splitext(os.path.basename(json_file))[0]
    job_output_dir = os.path.join(output_dir, base_name)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(job_output_dir, exist_ok=True)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    if gpu_id is not None:
        env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        gpu_info = f"GPU {gpu_id}"
    else:
        env.pop('CUDA_VISIBLE_DEVICES', None)
        gpu_info = "CPU"
    
    # æ„å»ºå‘½ä»¤ï¼ˆå‚ç…§å®˜æ–¹ CLIï¼‰
    cmd = ['python', af3_script,
           '--json_path', json_file,
           '--model_dir', model_dir,
           '--output_dir', job_output_dir]
    if public_db_dir:
        cmd += ['--public_db_dir', public_db_dir]
    # æ§åˆ¶æ˜¯å¦è¿è¡Œæ•°æ®æµæ°´çº¿ä¸æ¨ç†
    cmd += ['--run_data_pipeline', 'true' if run_data_pipeline else 'false']
    cmd += ['--run_inference', 'true' if run_inference else 'false']
    if extra_args:
        cmd += list(extra_args)
    
    start_time = time.time()
    logging.info(f"å¼€å§‹é¢„æµ‹: {base_name} -> {gpu_info}")
    
    try:
        # è¿è¡Œé¢„æµ‹
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            logging.info(f"âœ… é¢„æµ‹å®Œæˆ: {base_name} (è€—æ—¶: {duration:.1f}s)")
            return {
                'status': 'success',
                'sample': base_name,
                'output_dir': job_output_dir,
                'duration': duration,
                'gpu_info': gpu_info
            }
        else:
            logging.error(f"âŒ é¢„æµ‹å¤±è´¥: {base_name}")
            logging.error(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return {
                'status': 'failed',
                'sample': base_name,
                'error': result.stderr,
                'duration': duration,
                'gpu_info': gpu_info
            }
            
    except subprocess.TimeoutExpired:
        logging.error(f"â° é¢„æµ‹è¶…æ—¶: {base_name}")
        return {
            'status': 'timeout',
            'sample': base_name,
            'error': 'Prediction timeout',
            'gpu_info': gpu_info
        }
    except Exception as e:
        logging.error(f"âŒ é¢„æµ‹å¼‚å¸¸: {base_name} - {e}")
        return {
            'status': 'error',
            'sample': base_name,
            'error': str(e),
            'gpu_info': gpu_info
        }


def run_batch_predictions(
    input_dir: str,
    output_dir: str,
    af3_script: str,
    model_dir: str,
    public_db_dir: Optional[str] = None,
    max_concurrent: int = 4,
    run_data_pipeline: bool = True,
    run_inference: bool = True,
    verbose: bool = False,
    extra_args: Optional[List[str]] = None,
) -> Dict[str, List]:
    """
    æ‰¹é‡è¿è¡ŒAlphaFold3é¢„æµ‹
    
    Args:
        input_dir: JSONæ–‡ä»¶è¾“å…¥ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        af3_script: AF3è„šæœ¬è·¯å¾„
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        max_template_date: æœ€å¤§æ¨¡æ¿æ—¥æœŸ
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        
    Returns:
        æ‰¹é‡è¿è¡Œç»“æœç»Ÿè®¡
    """
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.isdir(input_dir):
        logging.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return {'success': [], 'failed': [], 'errors': []}
    
    # æŸ¥æ‰¾JSONæ–‡ä»¶
    json_files = []
    for file in os.listdir(input_dir):
        if file.endswith('.json'):
            json_path = os.path.join(input_dir, file)
            if validate_json_file(json_path):
                json_files.append(json_path)
            else:
                logging.warning(f"è·³è¿‡æ— æ•ˆJSONæ–‡ä»¶: {file}")
    
    if not json_files:
        logging.error(f"åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ–‡ä»¶")
        return {'success': [], 'failed': [], 'errors': []}
    
    logging.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªæœ‰æ•ˆJSONæ–‡ä»¶")
    
    # æ£€æµ‹GPU
    gpu_count = detect_gpu_count()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å‡†å¤‡ä»»åŠ¡
    tasks = []
    gpu_index = 0
    
    for json_file in json_files:
        if gpu_count > 0:
            gpu_id = gpu_index % gpu_count
            gpu_index += 1
        else:
            gpu_id = None
        
        tasks.append((json_file, gpu_id))
    
    # è¿è¡Œä»»åŠ¡
    results = {'success': [], 'failed': [], 'errors': []}
    
    if gpu_count > 0:
        # GPUæ¨¡å¼ï¼šæŒ‰GPUæ•°é‡é™åˆ¶å¹¶å‘
        max_workers = min(max_concurrent, gpu_count)
    else:
        # CPUæ¨¡å¼ï¼šæŒ‰CPUæ ¸å¿ƒæ•°é™åˆ¶å¹¶å‘
        max_workers = min(max_concurrent, os.cpu_count() or 4)
    
    logging.info(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘å·¥ä½œè¿›ç¨‹")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(
                run_single_prediction,
                json_file,
                output_dir,
                af3_script,
                model_dir,
                public_db_dir,
                run_data_pipeline,
                run_inference,
                gpu_id,
                extra_args
            ): (json_file, gpu_id)
            for json_file, gpu_id in tasks
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_task):
            json_file, gpu_id = future_to_task[future]
            try:
                result = future.result()
                
                if result['status'] == 'success':
                    results['success'].append(result)
                else:
                    results['failed'].append(result)
                    
            except Exception as e:
                logging.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {json_file} - {e}")
                results['errors'].append({
                    'sample': os.path.splitext(os.path.basename(json_file))[0],
                    'error': str(e)
                })
    
    return results


def generate_summary_report(results: Dict[str, List], output_dir: str):
    """ç”Ÿæˆè¿è¡Œæ€»ç»“æŠ¥å‘Š"""
    report_file = os.path.join(output_dir, "run_summary.txt")
    
    total_tasks = len(results['success']) + len(results['failed']) + len(results['errors'])
    success_count = len(results['success'])
    failed_count = len(results['failed'])
    error_count = len(results['errors'])
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        f.write("  AlphaFold3 æ‰¹é‡è¿è¡Œæ€»ç»“æŠ¥å‘Š\n")
        f.write("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
        f.write(f"è¿è¡Œæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è¾“å‡ºç›®å½•: {output_dir}\n\n")
        
        f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
        f.write("è¿è¡Œç»Ÿè®¡:\n")
        f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
        f.write(f"æ€»ä»»åŠ¡æ•°: {total_tasks}\n")
        f.write(f"æˆåŠŸ: {success_count}\n")
        f.write(f"å¤±è´¥: {failed_count}\n")
        f.write(f"é”™è¯¯: {error_count}\n")
        f.write(f"æˆåŠŸç‡: {success_count/total_tasks*100:.1f}%\n\n")
        
        if results['success']:
            f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
            f.write("æˆåŠŸæ ·æœ¬:\n")
            f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
            for result in results['success']:
                f.write(f"âœ… {result['sample']} ({result['gpu_info']}, {result['duration']:.1f}s)\n")
            f.write("\n")
        
        if results['failed']:
            f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
            f.write("å¤±è´¥æ ·æœ¬:\n")
            f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
            for result in results['failed']:
                f.write(f"âŒ {result['sample']} ({result['gpu_info']}) - {result.get('error', 'Unknown error')}\n")
            f.write("\n")
        
        if results['errors']:
            f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
            f.write("é”™è¯¯æ ·æœ¬:\n")
            f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
            for result in results['errors']:
                f.write(f"âš ï¸  {result['sample']} - {result['error']}\n")
    
    logging.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    parser = argparse.ArgumentParser(
        description="AlphaFold3 æ‰¹é‡è¿è¡Œå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬æ‰¹é‡è¿è¡Œ
  python %(prog)s --input_dir ./inputs --output_dir ./results
  
  # æŒ‡å®šå¹¶å‘æ•°å’Œæ¨¡æ¿æ—¥æœŸ
  python %(prog)s --input_dir ./inputs --output_dir ./results --max_concurrent 8 --max_template_date 2023-12-01
  
  # è¯¦ç»†æ—¥å¿—
  python %(prog)s --input_dir ./inputs --output_dir ./results --verbose
        """
    )
    parser.add_argument(
        "--input_dir", 
        required=True, 
        help="JSONæ–‡ä»¶è¾“å…¥ç›®å½•"
    )
    parser.add_argument(
        "--output_dir", 
        required=True, 
        help="é¢„æµ‹ç»“æœè¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--af3_script",
        default="/data/AlphaFold/alphafold3/run_alphafold.py",
        help="AlphaFold3è„šæœ¬è·¯å¾„ï¼ˆé»˜è®¤: /data/AlphaFold/alphafold3/run_alphafold.pyï¼‰"
    )
    parser.add_argument(
        "--model_dir",
        required=True,
        help="AlphaFold3 æ¨¡å‹å‚æ•°ç›®å½•ï¼ˆå¿…å¡«ï¼‰"
    )
    parser.add_argument(
        "--public_db_dir",
        help="å…¬å…±æ•°æ®åº“ç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºæ•°æ®æµæ°´çº¿ï¼‰"
    )
    parser.add_argument(
        "--max_concurrent",
        type=int,
        default=4,
        help="æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼ˆé»˜è®¤: 4ï¼‰"
    )
    parser.add_argument(
        "--run_data_pipeline",
        dest="run_data_pipeline",
        action="store_true",
        help="è¿è¡Œæ•°æ®æµæ°´çº¿ï¼ˆé»˜è®¤å¼€å¯ï¼‰"
    )
    parser.add_argument(
        "--no_run_data_pipeline",
        dest="run_data_pipeline",
        action="store_false",
        help="ç¦ç”¨æ•°æ®æµæ°´çº¿"
    )
    parser.set_defaults(run_data_pipeline=True)
    parser.add_argument(
        "--run_inference",
        dest="run_inference",
        action="store_true",
        help="è¿è¡Œæ¨ç†ï¼ˆé»˜è®¤å¼€å¯ï¼‰"
    )
    parser.add_argument(
        "--no_run_inference",
        dest="run_inference",
        action="store_false",
        help="ç¦ç”¨æ¨ç†"
    )
    parser.set_defaults(run_inference=True)
    parser.add_argument(
        "--extra_args",
        nargs=argparse.REMAINDER,
        help="ä¼ é€’ç»™ run_alphafold.py çš„å…¶ä»–å‚æ•°ï¼ˆæ”¾åœ¨ -- åï¼‰"
    )
    parser.add_argument(
        "--verbose", 
        "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ä¿¡æ¯"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)
    
    print("ğŸš€ å¼€å§‹ AlphaFold3 æ‰¹é‡é¢„æµ‹...\n")
    
    # æ£€æŸ¥AF3è„šæœ¬
    if not os.path.exists(args.af3_script):
        logging.error(f"AlphaFold3è„šæœ¬ä¸å­˜åœ¨: {args.af3_script}")
        sys.exit(1)
    
    # è¿è¡Œæ‰¹é‡é¢„æµ‹
    results = run_batch_predictions(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        af3_script=args.af3_script,
        model_dir=args.model_dir,
        public_db_dir=args.public_db_dir,
        max_concurrent=args.max_concurrent,
        run_data_pipeline=args.run_data_pipeline,
        run_inference=args.run_inference,
        verbose=args.verbose,
        extra_args=(args.extra_args or [])
    )
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_summary_report(results, args.output_dir)
    
    # æ‰“å°æ€»ç»“
    total = len(results['success']) + len(results['failed']) + len(results['errors'])
    success = len(results['success'])
    
    print(f"\nâœ… æ‰¹é‡é¢„æµ‹å®Œæˆï¼")
    print(f"ğŸ“Š æ€»ä»»åŠ¡: {total}, æˆåŠŸ: {success}, æˆåŠŸç‡: {success/total*100:.1f}%")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {os.path.join(args.output_dir, 'run_summary.txt')}")


if __name__ == "__main__":
    main()
